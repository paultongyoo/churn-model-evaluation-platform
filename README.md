# Churn Prediction Model Evaluation Pipeline

(For those evaluating this project for the [DataTalks.club MLOps Zoomcamp certification](https://datatalks.club/blog/mlops-zoomcamp.html), scroll down to the **"DataTalks.club MLOps Zoomcamp Evaluation Criteria" section** for abridged explanations of how this project sought to fulfill each criteria.)

## Problem Statement

* TODO

## Pipeline Target User Roles

* TODO

## Pipeline Flowchart

* TODO

## Pipeline Infrastructure Diagram

* TODO

## "Non-Production Use" Disclaimer

* TODO

## Customer Churn Data

* The labeled customer churn data used to train the model was randomly collected from an Iranian telecom company on 4/8/2020 and made available to download by the UC Irvine Machine Learning Repository at https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset .
* The `mlops-churn-pipeline` repository contains a `data` folder with several CSV files prefixed with the string `customer_churn_*`.
* The following files were split from the original Iranian Telecom dataset:
   * `customer_churn_0.csv`
   * `customer_churn_1.csv`
   * `customer_churn_2_majority_drifted.csv`
* The `customer_churn_synthetic_*.csv` files were generated by Gretel.ai using the `customer_churn_0/1/2*.csv` files as input.

## Prerequisites

* AWS Account
    * AWS Account required to run the pipeline to the cloud as a user
    * AWS Account NOT required to run unit and integration tests
* AWS User with the following Permissions:
    * TBD
* AWS CLI installed with `aws configure` run to store credentials locally
* Docker installed and Docker Engine running
* Pip and Pipenv installed
* Terraform installed

## Docker Local Image Storage Space Requirements

 About 1.8GB of disk space is required to store the following Docker images locally before deploying to AWS Elastic Container Repositories (ECR):
* **Custom Grafana Image**
    * Packages database configuration and dashboard files
    * Uses Grafana `grafana/grafana-enterprise:12.0.2-security-01` image
* **S3-to-Prefect Lambda Function**
    * Invokes orchestration flow when new files are dropped into S3
    * Uses AWS `public.ecr.aws/lambda/python:3.12` image

## How to Set Up Pipeline

1.  Install prerequisites (see list above)
1.  Create an S3 bucket to store the state of your Terraform infrastructure (e.g. `mlops-churn-pipeline-tf-state-<some random number>`)
1.  Clone `mlops-churn-pipeline` repository locally
1.  Edit root Terraform configuration to store state within S3
    1.  Edit `mlops-churn-pipeline/infrastructure/main.tf`
    1.  Change `terraform.backend.s3.bucket` to the name of the bucket you created
    1.  Change `terraform.backend.s3.region` to your AWS region
1.  Copy Terraform `stg.template.tfvars` file to new `stg.tfvars` file and define values for each key within:

| **Key Name** | **Purpose** | **Example Value** |
| ------------ | ----------- | ----------------- |
| `project_id` | Used as name for many AWS resources created by Terraform to avoid naming collisions, including the S3 bucket while files will be dropped and generated.  Choose something unique to avoid S3 bucket naming collisions. | `mlops-churn-pipeline-1349094` |
| `vpc_id` | Your AWS VPC ID | `vpc-0a1b2c3d4e5f6g7h8` |
| `aws_region` | Your AWS Region | `us-east-2` |
| `db_username` | Username for Postgres database used to store MLflow, Prefect, and Evidently Metrics.  Must conform to Postgres rules (e.g. lowercase, numbers, underscores only) | `my_super_secure_db_name` |
| `db_password` | Password for Postgres database. Use best practices and avoid spaces. | `Th1s1sAStr0ng#Pwd!` |
| `grafana_admin_user` | Username for Grafana account used to **edit** data drift and model prediction scores over time.  | `grafana_FTW` |
| `grafana_admin_password` | Password for Grafana account | `Grafana4Lyfe!123` |
| `subnet_ids`  | AWS Subnet IDs: **Must be public subnet IDs to allow Postgres RDS instance to be accessed by ECS services | `["subnet-123abc456def78901", "subnet-234bcd567efg89012"]` |
| `my_ip` | IP address that will be granted access to Grafana UI and Postgres DB | `203.0.113.42` |
| `my_email_address` | Email address that will be notified if input files exhibit data drift or prediction scores that exceed thresholds | `	your.name@example.com` |

1.  `cd {REPO_HOME}/code/orchestration` then `pipenv shell`
1.  Run `make plan` and review the infrastructure to be created (see diagram above for summary)
1.  Run `make apply` to build Terraform infrastructure, set Prefect Secrets, update GitHub Actions workflow, and start ECS services
1.  Click each of the 4 ECS Service URLs to confirm they are running: MLFlow, Prefect Server, Evidently, Grafana
1.  ` cd {REPO_HOME}` then `make model-registry` to train `XGBoostChurnModel` churn model and upload to MLFlow model registry with `staging` alias.
    1.  Confirm it was created by visiting the Model Registry with the MLFlow UI
1.  Deploy the `churn_prediction_pipeline` Prefect Flow to your Prefect Server using GitHub Actions
    1. Commit your cloned repo (including `{REPO_HOME}/.github/workflows/deploy-prefect.yml` updated with generated `PREFECT_API_URL`)
    1. Log in your GitHub account, access your committed repo project and create the following Repository Secrets (used by `deploy-prefect.yml`):
        1.  `AWS_ACCOUNT_ID`
        1.  `AWS_ACCESS_KEY_ID`
        1.  `AWS_SECRET_ACCESS_KEY`
        1.  `AWS_REGION`
    1.  Nagivate to GitHub Project Actions tab, select the workflow `Build and Deploy Prefect Flow to ECR`, and verify it completes successfully.
2.  Confirm your email subscription to the pipeline SNS topic
  3.  Navigate to your email inbox and look for an email subject titled `AWS Notification - Subscription Confirmation`.
  4.  Open the email and click the `Confirm Subscription` link within.
  5.  You should subsequently see a green message relaying your subscription has been confirmed.

## Steps to Run the Pipeline and evaluate the Customer Churn model

1.  Navigate to `{REPO_HOME}` (and run `cd {REPO/HOME}code/orchestration && pipenv shell` if you haven't already)
2.  You can process the labeled Customer Churn data in one of two ways:
   1.   Manually upload files from the `{REPO_HOME}/data` folder into the S3 bucket `{PROJECT_ID}/data/input` folder
   2.   Run `make simulate-file-drops` from `{REPO_HOME}` to run the script `upload_simulation_script.py` which uploads each file in the `data` folder (except `customer_churn_0.csv`) to the S3 bucket folder to more conveniently plot and review metrics changing over time in Grafana.

## Pipeline Logs

* TODO

## DataTalks.club MLOps Zoomcamp Evaluation Criteria
Source: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/07-project

* Problem description
    * 0 points: The problem is not described
    * 1 point: The problem is described but shortly or not clearly 
    * [✔️] 2 points: The problem is well described and it's clear what the problem the project solves
* Cloud
    * 0 points: Cloud is not used, things run only locally
    * 2 points: The project is developed on the cloud OR uses localstack (or similar tool) OR the project is deployed to Kubernetes or similar container management platforms
    * [✔️] 4 points: The project is developed on the cloud and IaC tools are used for provisioning the infrastructure
* Experiment tracking and model registry
    * 0 points: No experiment tracking or model registry
    * 2 points: Experiments are tracked or models are registered in the registry
    * [✔️] 4 points: Both experiment tracking and model registry are used
* Workflow orchestration
    * 0 points: No workflow orchestration
    * 2 points: Basic workflow orchestration
    * [✔️] 4 points: Fully deployed workflow 
* Model deployment
    * 0 points: Model is not deployed
    * 2 points: Model is deployed but only locally
    * [✔️] 4 points: The model deployment code is containerized and could be deployed to cloud or special tools for model deployment are used
* Model monitoring
    * 0 points: No model monitoring
    * 2 points: Basic model monitoring that calculates and reports metrics
    * [✔️] 4 points: Comprehensive model monitoring that sends alerts or runs a conditional workflow (e.g. retraining, generating debugging dashboard, switching to a different model) if the defined metrics threshold is violated
* Reproducibility
    * 0 points: No instructions on how to run the code at all, the data is missing
    * 2 points: Some instructions are there, but they are not complete OR instructions are clear and complete, the code works, but the data is missing
    * [✔️] 4 points: Instructions are clear, it's easy to run the code, and it works. The versions for all the dependencies are specified.
* Best practices
    * [✔️] There are unit tests (1 point)
    * [✔️] There is an integration test (1 point)
    * [✔️] Linter and/or code formatter are used (1 point)
    * [✔️] There's a Makefile (1 point)
    * [✔️] There are pre-commit hooks (1 point)
    * [✔️] There's a CI/CD pipeline (2 points)
