# Churn Prediction Model Evaluation Pipeline

(For those evaluating this project for the [DataTalks.club MLOps Zoomcamp certification](https://datatalks.club/blog/mlops-zoomcamp.html), scroll down to the **"DataTalks.club MLOps Zoomcamp Evaluation Criteria" section** for abridged explanations of how this project sought to fulfill each criteria.)

## Problem Statement

* TODO

## Customer Churn Data Source

* The labeled customer churn data used to train the model was randomly collected from an Iranian telecom company on 4/8/2020 and made available to download by the UC Irvine Machine Learning Repository at https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset .
* The `mlops-churn-pipeline` repository contains a `data` folder with several CSV files prefixed with the string `customer_churn_*`.
* The following files were split from the original Iranian Telecom dataset:
   * `customer_churn_0.csv`
   * `customer_churn_1.csv`
   * `customer_churn_2_majority_drifted.csv`
* The `customer_churn_synthetic_*.csv` files were generated by Gretel.ai using the `customer_churn_0/1/2*.csv` files as input.

## Pipeline Target User Roles

* Machine Learning Operations (MLOps) Engineer
* Machine Learning Engineer
* Data Scientist

## Pipeline Flowchart

* TODO

## "Non-Production Use" Disclaimer

* TODO

## Pipeline Infrastructure Diagram

* TODO

## Project Folder Structure

* TODO

## Prerequisites

* [AWS Account](https://aws.amazon.com/)
    * AWS Account required to run the pipeline to the cloud as a user
    * AWS Account NOT required to run unit and integration tests
* AWS User with the following [Identity & Access Management(IAM) Permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html):
    * TBD
* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) installed with `aws configure` run to store credentials locally
* [Docker](https://docs.docker.com/get-started/get-docker/) installed and Docker Engine running
* [Pip](https://pip.pypa.io/en/stable/installation/) and [Pipenv](https://pipenv.pypa.io/en/latest/) installed
* [Terraform](https://developer.hashicorp.com/terraform/install) installed

## Docker Local Image Storage Space Requirements

 About 1.8GB of disk space is required to store the following Docker images locally before deploying to AWS Elastic Container Repositories (ECR):
* **Custom Grafana Image**
    * Packages database configuration and dashboard files
    * Uses Grafana `grafana/grafana-enterprise:12.0.2-security-01` image
* **S3-to-Prefect Lambda Function**
    * Invokes orchestration flow when new files are dropped into S3
    * Uses AWS `public.ecr.aws/lambda/python:3.12` image

## How to Set Up Pipeline

1.  Install prerequisites (see list above)
1.  Create an S3 bucket to store the state of your Terraform infrastructure (e.g. `mlops-churn-pipeline-tf-state-<some random number>`)
1.  Clone `mlops-churn-pipeline` repository locally
1.  Edit root Terraform configuration to store state within S3
    1.  Edit `mlops-churn-pipeline/infrastructure/main.tf`
    1.  Change `terraform.backend.s3.bucket` to the name of the bucket you created
    1.  Change `terraform.backend.s3.region` to your AWS region
1.  Copy Terraform `stg.template.tfvars` file to new `stg.tfvars` file and define values for each key within:

| **Key Name** | **Purpose** | **Example Value** |
| ------------ | ----------- | ----------------- |
| `project_id` | Used as name for many AWS resources created by Terraform to avoid naming collisions, including the S3 bucket while files will be dropped and generated.  Choose something unique to avoid S3 bucket naming collisions. | `mlops-churn-pipeline-1349094` |
| `vpc_id` | Your AWS VPC ID | `vpc-0a1b2c3d4e5f6g7h8` |
| `aws_region` | Your AWS Region | `us-east-2` |
| `db_username` | Username for Postgres database used to store MLflow, Prefect, and Evidently Metrics.  Must conform to Postgres rules (e.g. lowercase, numbers, underscores only) | `my_super_secure_db_name` |
| `db_password` | Password for Postgres database. Use best practices and avoid spaces. | `Th1s1sAStr0ng#Pwd!` |
| `grafana_admin_user` | Username for Grafana account used to **edit** data drift and model prediction scores over time.  | `grafana_FTW` |
| `grafana_admin_password` | Password for Grafana account | `Grafana4Lyfe!123` |
| `subnet_ids`  | AWS Subnet IDs: **Must be public subnet IDs to allow Postgres RDS instance to be accessed by ECS services** | `["subnet-123abc456def78901", "subnet-234bcd567efg89012"]` |
| `my_ip` | IP address that will be granted access to Grafana UI and Postgres DB | `203.0.113.42` |
| `my_email_address` | Email address that will be notified if input files exhibit data drift or prediction scores that exceed thresholds | `	your.name@example.com` |

1.  `cd {REPO_HOME}/code/orchestration` then `pipenv shell`
1.  Run `make plan` and review the infrastructure to be created (see diagram above for summary)
1.  Run `make apply` to build Terraform infrastructure, set Prefect Secrets, update GitHub Actions workflow, and start ECS services
1.  Click each of the 4 ECS Service URLs to confirm they are running: MLFlow, Prefect Server, Evidently, Grafana
1.  ` cd {REPO_HOME}` then `make model-registry` to train `XGBoostChurnModel` churn model and upload to MLFlow model registry with `staging` alias.
    1.  Confirm it was created by visiting the Model Registry with the MLFlow UI
1.  Deploy the `churn_prediction_pipeline` Prefect Flow to your Prefect Server using GitHub Actions
    1. Commit your cloned repo (including `{REPO_HOME}/.github/workflows/deploy-prefect.yml` updated with generated `PREFECT_API_URL`)
    1. Log in your GitHub account, navigate to your committed repo project and create the following Repository Secrets (used by `deploy-prefect.yml`):
        1.  `AWS_ACCOUNT_ID`
        1.  `AWS_ACCESS_KEY_ID`
        1.  `AWS_SECRET_ACCESS_KEY`
        1.  `AWS_REGION`
    1.  Nagivate to GitHub Project Actions tab, select the workflow `Build and Deploy Prefect Flow to ECR`, and verify it completes successfully.
2.  Confirm your email subscription to the pipeline SNS topic
  3.  Navigate to your email inbox and look for an email subject titled `AWS Notification - Subscription Confirmation`.
  4.  Open the email and click the `Confirm Subscription` link within.
  5.  You should subsequently see a green message relaying your subscription has been confirmed.

## Summary of Pipeline Services Created

Once the Terraform `make apply` command completes successfully, you should see output similar to the following that provides you URLs to each of the 4 tools created:
```
üéâ All systems go! üéâ

MLflow, Prefect, Evidently, and Grafana UI URLs
-----------------------------------------------

üß™ MLflow UI: http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:5000
‚öôÔ∏è Prefect UI: http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:4200
üìà Evidently UI: http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:8000
üìà Grafana UI: http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:3000
```

Clicking on each URL *should* render each tool's UI successfully in your browser (the Terraform command includes invoking a script that polls the services' URLs until they return successful responses).

These URLs were also written to the `{REPO_HOME}/.env` file for future retrieval and export to shell environment when needed.

```
MLFLOW_TRACKING_URI=http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:5000
PREFECT_API_URL=http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:4200/api
EVIDENTLY_UI_URL=http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:8000
PREFECT_UI_URL=http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:4200
GRAFANA_UI_URL=http://mlops-churn-pipeline-alb-123456789.us-east-2.elb.amazonaws.com:3000
```

The following sections give a brief overview of the tool features made available in this project.

### MLFlow Tracking Server & Model Registry
TODO

### Prefect Orchestration Server and Worker Service
TODO

### Evidently Non-Time-Series Dashboard and Reports UI
TODO

### Grafana Time-Series Dashboard UI
TODO

## How to Generate Churn Model Evaluation Metrics

1.  Navigate to `{REPO_HOME}` (and run `cd {REPO/HOME}code/orchestration && pipenv shell` if you haven't already)
2.  You can process the labeled Customer Churn data in one of two ways:
   1.   Manually upload files from the `{REPO_HOME}/data` folder into the S3 bucket `{PROJECT_ID}/data/input` folder
   2.   Run `make simulate-file-drops` from `{REPO_HOME}` to run the script `upload_simulation_script.py` which uploads each file in the `data` folder (except `customer_churn_0.csv`) to the S3 bucket folder to more conveniently plot and review metrics changing over time in Grafana.
3.  Once you drop files into the S3 bucket `{PROJECT_ID}/data/input` folder, you can navigate to 

## Pipeline Logs

* TODO

## DataTalks.club MLOps Zoomcamp Evaluation Criteria
Source: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/07-project

### Problem description
#### Target: The problem is well described and it's clear what the problem the project solves
TODO

### Cloud
#### Target: The project is developed on the cloud and IaC tools are used for provisioning the infrastructure
TODO

### Experiment tracking and model registry
#### Target: Both experiment tracking and model registry are used
TODO

### Workflow orchestration
#### Target: Fully deployed workflow 
TODO

### Model deployment
#### Target: The model deployment code is containerized and could be deployed to cloud or special tools for model deployment are used
TODO

### Model monitoring
#### Target: Comprehensive model monitoring that sends alerts or runs a conditional workflow (e.g. retraining, generating debugging dashboard, switching to a different model) if the defined metrics threshold is violated
TODO

### Reproducibility
#### Target: Instructions are clear, it's easy to run the code, and it works. The versions for all the dependencies are specified.
TODO

### Best practices
#### Target: There are unit tests (1 point)
TODO

#### Target: There is an integration test (1 point)
TODO

#### Target: Linter and/or code formatter are used (1 point)
TODO

#### Target: There's a Makefile (1 point)
TODO

#### Target: There are pre-commit hooks (1 point)
TODO

#### Target: There's a CI/CD pipeline (2 points)
TODO

